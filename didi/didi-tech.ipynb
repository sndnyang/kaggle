{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 滴滴比赛\n",
    "\n",
    "导入必须的！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "base_dir = \"G:/project/dataset/citydata/season_1/training_data\"\n",
    "\n",
    "train_cluster_map_file = os.path.join(base_dir, 'cluster_map', 'cluster_map')\n",
    "train_poi_file = os.path.join(base_dir, 'poi_data', 'poi_data')\n",
    "train_order_file = os.path.join(base_dir, 'order_data', 'order_data_2016-01-21')\n",
    "#train_cluster_map = os.path.join(base_dir, 'cluster_map')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 区域 hashid 和 实际id\n",
    "\n",
    "cluster_map 数据， pandas 应该比 dict 快， 到时候 merge 或 join 比 从 dict 里找好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_map = pd.read_csv(train_cluster_map_file, sep=\"\\t\", header=None, names=['hashid', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print len(cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashid</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90c5a34f06ac86aee0fd70e2adce7d8a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2c8c4bb99e6377d21de71275afd6cd2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             hashid  id\n",
       "0  90c5a34f06ac86aee0fd70e2adce7d8a   1\n",
       "1  f2c8c4bb99e6377d21de71275afd6cd2   2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_map[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复习下 pandas 读数据\n",
    "\n",
    "1. 复习下数据读取, read_csv\n",
    "2. 个人认为不需要的数据， 如 order_id 等id， 该省略的省掉（order_id, passenger_id, dest_district_has）， 该转换的转换, driver_id 变成是否有接单， 时间拆出日期和时间片， start_distrct_hash 改成实际id。 节省空间。去掉一堆 hash的字符串， 可以省很多空间\n",
    "\n",
    "**也许有些信息还有用， 觉得有用时再补充回来**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 时：分：秒 转成 10分钟的时间片\n",
    "def time2Slice(time_str):\n",
    "    l = time_str.split(\":\")\n",
    "    return int(l[0]) * 6 + int(l[1]) / 10 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order data 的数据 hash 太多， 顺便复习下 pandas\n",
    "def reduceOrderFile(base_name, f):\n",
    "    import time\n",
    "    prev = time.time()\n",
    "    fname = os.path.join(base_name, f)\n",
    "    order = pd.read_csv(fname, sep=\"\\t\", header=None, dtype={'Price': np.str},\n",
    "                          names=['order_id', 'driver_id', 'passenger_id', 'start_district_hash', 'dest_district_hash',\n",
    "                                'Price', 'Time'])\n",
    "\n",
    "    full_order = order.merge(cluster_map, left_on='start_district_hash', right_on='hashid')\n",
    "\n",
    "    full_order['Date'] = pd.Series([e.split()[0] for e in full_order['Time']])\n",
    "\n",
    "    full_order['TimePiece'] = pd.Series([time2Slice(e.split()[1]) for e in full_order['Time']])\n",
    "\n",
    "    full_order['Resp'] = full_order['driver_id'].notnull()\n",
    "\n",
    "    small_order = full_order.loc[:, ['Date', 'TimePiece', 'id', 'Price', 'Resp']]\n",
    "    small_order_file = os.path.join(base_name, f + \".csv\")\n",
    "    small_order.to_csv(small_order_file, index = False)\n",
    "    print time.time() - prev\n",
    "    \n",
    "    #return small_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.23000001907\n"
     ]
    }
   ],
   "source": [
    "# 测试函数\n",
    "\n",
    "def testReduceOrder():\n",
    "    z = reduceOrderFile('G:/project/dataset/citydata/season_1/training_data\\order_data', 'order_data_2016-01-21')\n",
    "    print len(z)\n",
    "    print sum(z['Resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 批量 order reduce\n",
    "def reduceFiles(base_name):\n",
    "    for rt, dirs, files in os.walk(base_name):\n",
    "        for f in files:           \n",
    "            reduceOrderFile(base_name, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.16599988937\n",
      "4.09300017357\n",
      "4.24799990654\n",
      "4.88099980354\n",
      "5.09999990463\n",
      "4.90000009537\n",
      "4.94000005722\n",
      "5.15599989891\n",
      "5.15200018883\n",
      "5.24800014496\n",
      "6.0119998455\n",
      "5.132999897\n",
      "5.16599988937\n",
      "5.06999993324\n",
      "5.88199996948\n",
      "6.375\n",
      "4.98500013351\n",
      "5.53900003433\n",
      "6.41299986839\n",
      "5.78500008583\n",
      "5.19200015068\n",
      "0.603999853134\n"
     ]
    }
   ],
   "source": [
    "# 运行过就不要再来了， 一个文件6秒， 20个文件， 还是要2分钟的。\n",
    "def runForOnce():\n",
    "    reduceFiles(os.path.join(base_dir, 'order_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_read_order():\n",
    "    train_order = pd.read_csv(train_order_file, sep=\"\\t\", header=None, dtype={'Price': np.float, 'driver_id': np.str},\n",
    "                          names=['order_id', 'driver_id', 'passenger_id', 'start_district_hash', 'dest_district_hash',\n",
    "                                'Price', 'Time'])\n",
    "    print len(train_order)\n",
    "    t = train_order['driver_id']\n",
    "    print t.notnull()\n",
    "    small_order = full_train_order.loc[:, ['Date', 'TimePiece', 'id', 'Price', 'Resp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474340\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
